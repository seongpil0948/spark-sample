# Jupyter Notebooks for Spark S3 Examples

ì´ ë””ë ‰í† ë¦¬ì—ëŠ” Jupyter Labì—ì„œ Sparkì™€ S3ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ ë…¸íŠ¸ë¶ë“¤ì´ ìˆìŠµë‹ˆë‹¤.

## ë…¸íŠ¸ë¶ ëª©ë¡

### ğŸ“Š `s3_example.ipynb`
Sparkì™€ S3ë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë¶„ì„ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë³´ì—¬ì£¼ëŠ” ì¢…í•© ì˜ˆì œì…ë‹ˆë‹¤.

**ì£¼ìš” ë‚´ìš©:**
- SparkSession ìƒì„± ë° S3 ì„¤ì •
- S3ì—ì„œ ë°ì´í„° ì½ê¸°/ì“°ê¸°
- ë°ì´í„° ë¶„ì„ ë° ë³€í™˜
- SQL ì¿¼ë¦¬ ì‚¬ìš©
- Pandas ì—°ë™ ë° ì‹œê°í™”
- ì„±ëŠ¥ ìµœì í™” íŒ

## ì‚¬ìš© ë°©ë²•

### 1. í™˜ê²½ ì¤€ë¹„
```bash
# Docker Composeë¡œ Jupyter ì‹œì‘
docker-compose up -d jupyter

# ë˜ëŠ” ì „ì²´ ìŠ¤íƒ ì‹œì‘
./scripts/start-full-stack.sh
```

### 2. Jupyter Lab ì ‘ì†
ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8888/lab ì ‘ì†

### 3. S3 ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì„ íƒì‚¬í•­)
```bash
# S3ì— ìƒ˜í”Œ ë°ì´í„° ì—…ë¡œë“œ
python scripts/generate_sample_data.py --s3
```

### 4. ë…¸íŠ¸ë¶ ì‹¤í–‰
- `notebooks/s3_example.ipynb` íŒŒì¼ ì—´ê¸°
- ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰

## í•„ìˆ˜ ì¡°ê±´

### AWS ì„¤ì •
1. **AWS Profile ì„¤ì •**
   ```bash
   aws configure --profile toy-root
   ```

2. **í™˜ê²½ë³€ìˆ˜ í™•ì¸**
   ```bash
   echo $AWS_PROFILE  # toy-root
   ```

3. **S3 ì ‘ê·¼ ê¶Œí•œ**
   - ë²„í‚·: `theshop-lake-dev`
   - ì½ê¸°/ì“°ê¸° ê¶Œí•œ í•„ìš”

### Docker í™˜ê²½ë³€ìˆ˜
Docker Composeì—ì„œ ìë™ìœ¼ë¡œ ì„¤ì •ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë“¤:
```yaml
environment:
  - AWS_PROFILE=toy-root
  - AWS_REGION=us-east-1
  - SPARK_MASTER=spark://spark-master:7077
```

## S3 ê²½ë¡œ êµ¬ì¡°

**âš ï¸ ì¤‘ìš”**: Jupyterì™€ Sparkì—ì„œëŠ” ë°˜ë“œì‹œ `s3a://` í”„ë¡œí† ì½œì„ ì‚¬ìš©í•˜ì„¸ìš” (`s3://` ì•„ë‹˜)

```
s3a://theshop-lake-dev/
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â”œâ”€â”€ date=2024-01-01/
â”‚   â”‚   â”œâ”€â”€ date=2024-01-02/
â”‚   â”‚   â””â”€â”€ training/
â”‚   â”‚       â”œâ”€â”€ train/
â”‚   â”‚       â””â”€â”€ test/
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ user_summary/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ streaming/
â”‚   â””â”€â”€ checkpoint/
â”‚       â””â”€â”€ streaming/
```

## ì£¼ìš” ê¸°ëŠ¥

### 1. ë°ì´í„° ì½ê¸°
```python
# íŠ¹ì • ë‚ ì§œ ë°ì´í„° (s3a:// í”„ë¡œí† ì½œ ì‚¬ìš©)
df = spark.read.parquet("s3a://theshop-lake-dev/spark/input/date=2024-01-01/")

# ì—¬ëŸ¬ ë‚ ì§œ ë°ì´í„°
df = spark.read.parquet("s3a://theshop-lake-dev/spark/input/date=*/")

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš© (ê¶Œì¥)
from s3_utils import get_s3_input_path
df = spark.read.parquet(get_s3_input_path("2024-01-01"))
```

### 2. ë°ì´í„° ë¶„ì„
```python
# DataFrame API ì‚¬ìš©
result = df.groupBy("event_type").count()

# SQL ì‚¬ìš©
df.createOrReplaceTempView("events")
result = spark.sql("SELECT event_type, COUNT(*) FROM events GROUP BY event_type")
```

### 3. ë°ì´í„° ì €ì¥
```python
# S3ì— Parquet ì €ì¥ (s3a:// í”„ë¡œí† ì½œ ì‚¬ìš©)
df.write.mode("overwrite").parquet("s3a://theshop-lake-dev/spark/output/results/")

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš© (ê¶Œì¥)
from s3_utils import get_s3_output_path
df.write.mode("overwrite").parquet(get_s3_output_path("results"))
```

### 4. ì‹œê°í™”
```python
# Pandasë¡œ ë³€í™˜ í›„ matplotlib ì‚¬ìš©
pandas_df = spark_df.toPandas()
pandas_df.plot(kind='bar')
```

## ë¬¸ì œ í•´ê²°

### 1. S3 ì ‘ê·¼ ì˜¤ë¥˜
```bash
# AWS ìê²©ì¦ëª… í™•ì¸
aws sts get-caller-identity --profile toy-root

# S3 ì ‘ê·¼ í…ŒìŠ¤íŠ¸
aws s3 ls s3://theshop-lake-dev/ --profile toy-root
```

### 2. Spark ì—°ê²° ì˜¤ë¥˜
- Spark Master UI í™•ì¸: http://localhost:8080
- Jupyter ë¡œê·¸ í™•ì¸: `docker-compose logs jupyter`

### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •
df = df.repartition(200)

# ìºì‹œ ì •ë¦¬
spark.catalog.clearCache()
```

## ì„±ëŠ¥ íŒ

### 1. íŒŒí‹°ì…˜ ìµœì í™”
```python
# ì½ê¸° ì „ íŒŒí‹°ì…˜ ìˆ˜ ì„¤ì •
spark.conf.set("spark.sql.files.maxPartitionBytes", "128MB")

# ì“°ê¸° ì‹œ íŒŒí‹°ì…˜ ì¡°ì •
df.coalesce(1).write.parquet(path)
```

### 2. ìºì‹± í™œìš©
```python
# ìì£¼ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ìºì‹±
df.cache()
df.count()  # ìºì‹œ íŠ¸ë¦¬ê±°
```

### 3. ì»¬ëŸ¼ ì„ íƒ
```python
# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
df.select("user_id", "event_type", "amount")
```

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [Spark SQL ê°€ì´ë“œ](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [S3A ì„¤ì • ê°€ì´ë“œ](https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html)
- [PySpark API ë¬¸ì„œ](https://spark.apache.org/docs/latest/api/python/)